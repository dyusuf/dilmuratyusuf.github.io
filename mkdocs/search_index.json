{
    "docs": [
        {
            "location": "/",
            "text": "Introduction to Galaxy\n\n\nThe tutorial is the modified version which was written by \nSimon\nGladman\n - VLSCI\n\n\nBackground\n\n\nGalaxy is a web based analysis and workflow platform designed for\nbiologists to analyse their own data. It comes with most of the popular\nbioinformatics tools already installed and ready for use. There are many\nGalaxy servers around the world and some are tailored with specific\ntoolsets and reference data for analysis of human genomics, microbial\ngenomics, proteomics etc.\n\n\nThere are some introductory slides available\n\nhere\n.\n\n\nBasically, the Galaxy interface is separated into 3 parts. The tool list\non the left, the viewing pane in the middle and the analysis and data\nhistory on the right. We will be looking at all 3 parts in this\ntutorial.\n\n\n\n\nThis workshop/tutorial will familiarize you with the Galaxy interface.\nIt will cover the following topics:\n\n\n\n\nLogging in to the server\n\n\nGetting data into galaxy\n\n\nHow to access the tools\n\n\nUsing to use some common tools\n\n\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\nBe able to upload data to a Galaxy server from:\n    -   A file on your local computer\n    -   A file on a remote datastore with an accessible URL.\n    -   A file UCSC genome browser \nmissing\n\n\nBe able use tools in Galaxy by:\n    -   Accessing the tool via the tool menu\n    -   Using the tool interface to run the particular tool\n    -   Viewing/accessing the tool output.\n\n\n\n\nSection 1: Preparation.\n\n\nThe purpose of this section is to get you to log in to the server..\n\n\n\n\n\n\nGo to \nMDC galaxy production sever\n\n\n\n\n\n\nLog in:\n\n\n\n\nEnter your MDC user name\n\n\nEnter your password\n\n\nClick \nOK\n\n\n\n\n\n\n\n\n\n\nSection 2: Getting data into Galaxy\n\n\nThere are 2 main ways to get your data into Galaxy. We will use each of\nthese methods for 3 files and then work use those 3 files for the rest\nof the workshop.\n\n\n\n\n\n\nStart a new history for this workshop. To do this:\n\n\n\n\nClick on the history menu button (the\n    \n icon)\n    at the top of the Histories panel.\n\n\nSelect \nCreate New\n\n\n\n\n\n\n\n\nIt is important to note that Galaxy has the concept of \"File Type\" built\nin. This means that each file stored needs to have its type described to\nGalaxy as it is being made available. Examples of file types are: text,\nfasta, fastq, vcf, GFF, Genbank, tabular etc.\n\n\nWe will tell Galaxy what type of file each one is as we upload it.\n\n\nMethod 1: Upload a file from your own computer\n\n\nWith this method you can get most of the files on your own computer into\nGalaxy. (there is a size limit)\n\n\n\n\n\n\nDownload the following file to your computer: \nto change\n\n    \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz\n\n\n\n\nFrom the Galaxy tool panel, click on \nGet Data -> Upload\n    File\n\n\nClick the \nChoose File\n button\n\n\nFind and select the \nContig_stats.txt.gz\n file you downloaded\n    and click \nOpen\n\n\nSet the \"file format\" to \ntabular\n\n\nClick the \nStart\n button\n\n\nOnce the progress bar reaches 100%, click the \nClose\n button\n\n\n\n\n\n\n\n\nThe file will now upload to your current history.\n\n\nMethod 2: Upload a file from a URL\n\n\nIf a file exists on a web resource somewhere and you know its URL\n(Unique resource location - a web address) you can directly load it into\nGalaxy.\n\n\n\n\n\n\nFrom the tool panel, click on \nGet Data -> Upload File\n\n\n\n\nClick on the \nPaste/Fetch Data\n button\n\n\nCopy and paste the following web address into the URL/Text box: \nto change\n\n    \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz\n\n\nSet the file format to \nfastqsanger\n (not fastqcsanger)\n\n\nClick \nStart\n\n\nOnce the progress bar has reached 100%, click \nClose\n\n\n\n\n\n\n\n\nNote that Galaxy is smart enough to recognize that this is a compressed\nfile and so it will uncompress it as it loads it.\n\n\nMethod 2 (again): Get data from a public URL\n\n\nNow we are going to upload another file from the remote data source.\n\n\nRepeat the above for:\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna\n\n\nNote that this file is a \nfasta\n file and not a \nfastqsanger\n file.\n\n\nThe DNA sequence of \nStaphlococcus aureus MRSA252\n will be loaded into\nyour history as a fasta file.\n\n\nThe data\n\n\nThough we aren't going to focus on the contents of these files and what\nthey mean from a bioinformatics standpoint, here is a brief description\nof each one.\n\n\n\n\n\n\nContigs_stats.txt\n\n\n\n\nthis file contains a table of summary data from a de novo genome\n    assembly (the process of attempting to recover the full genome\n    of an organism from the short read sequences produced by most\n    DNA sequencing machines. )\n\n\nThe columns contain a lot of information but the ones we will be\n    using indicate the amount of data (or coverage) that went into\n    making up each piece of the final assembly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbacterial_std_err_1.fastq.gz\n\n\n\n\nThis file contains sequence reads as they would come off an\n    Illunina sequencing machine. They are in\n    \nfastq\n format.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMRSA0252.fna\n\n\n\n\nThis file contains the genome sequence of \nStaphylococcus aureus\n    MRSA252\n. It is in\n    \nfasta\n format.\n\n\n\n\n\n\n\n\n\n\nSection 3: Play with the tools\n\n\nThe purpose of this section is to get you used to using the available\ntools in Galaxy and point out some of the more basic manipulation tools.\n\n\nFirstly however, you\u2019ll notice that two of the files have very long and\nconfusing names. So we might want to change them. To do this we need to\n\u201cedit\u201d the file. So:\n\n\n\n\nClick on the\n    \n\n    icon (edit) next to the file in the history called:\n    \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq\n\n\nIn the \"Name\" text box, give it a new name. Call it: \nTypical Fastq\n    File\n\n\nClick the \nSave\n button.\n\n\n\n\nRepeat the process for the MRSA252 fasta file. Rename it to\n\nMRSA252.fna\n\n\nNow that\u2019s better. There was a lot of other functionality hidden behind\nthat edit\n(\n)\nicon. You can change a file\u2019s data type, convert its format and many\nother things. Feel free to play around with them.\n\n\nOk, back to the tools..\n\n\nExample 1: Histogram and summary statistics\n\n\nThe first thing we are going to do is produce a histogram of contig read\ncoverage depths and calculate the summary statistics from the\nContig_stats.txt file. To do this we need to cut out a couple of\ncolumns, remove a line and then produce a histogram. This will introduce\nsome of the text manipulation tools.\n\n\nClick on the\n\n\nicon of the \nContig_stats.txt\n file to have a look at it. Note that\nthere are 18 columns in this file. We want column 1 and column 6. To do\nthis:\n\n\n1. Cut out column 1 and column 6.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -> Cut\n and\n    set the following:\n\n\nSet \"Cut Columns\" to: \nc1,c6\n\n\n\"Delimited by\": \nTab\n\n\n\"Cut from\": \nContig_stats.txt\n\n\nClick \nExecute\n\n\n\n\nExamine the new file by clicking on it\u2019s\n\n\nicon. We now have 2 columns instead of the 18 in the original file.\n\n\n2. Remove the Header lines of the new file.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -> Remove\n    beginning\n and set the following:\n\n\n\"Remove First\": \n1\n\n\n\"from\": \nCut on data1\n\n\nclick \nExecute\n\n\n\n\nNote the the new file is the same as the previous one without the header\nline.\n\n\n3. Make a histogram.\n\n\n\n\nFrom the tool panel, click on \nGraph/Display Data -> Histogram\n\n    and set the following:\n\n\n\"Dataset\": \nRemove beginning on Data X\n\n\n\"Numerical column for X axis\": \nc2\n\n\n\"Number of breaks\": \n25\n\n\n\"Plot title\": \nHistogram of Contig Coverage\n\n\n\"Label for X axis\": \nCoverage depth\n\n\nClick \nExecute\n\n\n\n\nClick on the\n\n\nicon of the histogram to have a look at it. Note there are a few peaks..\nMaybe these correspond to single, double and triple copy number of these\ncontigs.\n\n\n4. Calculate summary statistics for contig coverage depth.\n\n\n\n\nFrom the tool panel, click on \nStatistics -> Summary\n    Statisitics\n and set the following:\n\n\n\"Summary statistics on\": \nRemove beginning on Data X\n\n\n\"Column or expression\": \nc2\n\n\nClick \nExecute\n\n\n\n\nYou\u2019ll note that the summary statistics tool failed and is red in the\nhistory. There was an error! If you click on the filename, and then the\nbug\n\n\nsymbol, it will tell you what went wrong. (There is a missing python\nlibrary.) At this point, you would normally contact your Galaxy server\nadministrator.\n\n\nExample 2: Convert Fastq to Fasta\n\n\nThis shows how to convert a fastq file to a fasta file. The tool creates\na new file with the converted data.\n\n\nConverter tool\n\n\n\n\nFrom the tool panel, click on \nConvert Formats -> FASTQ to\n    FASTA\n and set the following:\n\n\n\"FASTQ file to convert\": \nTypical Fastq File\n\n\nClick \nExecute\n\n\n\n\nThis will have created a new Fasta file called FASTQ to FASTA on data 2.\n\n\nExample 3: Find Ribosomal RNA Features in a DNA Sequence\n\n\nThis example shows how to use a tool called \u201cbarrnap\u201d to search for\nrRNAs in a DNA sequence.\n\n\n1. Find all of the ribosomal RNA's in a sequence\n\n\n\n\nFrom the tool panel, click on \nAnnotation -> barrnap\n and set\n    the following:\n\n\n\"Fasta file\": MRSA252.fna\n\n\nClick \nExecute\n\n\n\n\nA new file called \nbarrnap on data 3\n will be produced. It is a gff3\nfile. (This stands for genome feature format - version 3. It is a file\nformat for describing features contained by a DNA sequence.) Change it\u2019s\nname to something more appropriate (click on the\n\n\nicon.) There is also a STDERR output file from this tool - just ignore\nthis one.\n\n\nNow lets say you only want the lines of the file for the 23S rRNA\nannotations. We can do this using a Filter tool.\n\n\n2. Filter the annotations to get the 23S RNAs\n\n\n\n\nFrom the tool panel, click on \nFilter and Sort -> Select\n and\n    set the following:\n\n\n\"Select lines from\": (whatever you called the barrnap gff3 output)\n\n\n\"the pattern\": \n23S\n (this will look for all the lines in the file\n    that contain \u201c23S\u201d)\n\n\nClick \nExecute\n\n\n\n\nNow you have a gff3 file with just the 23S annotations!\n\n\nWhat now?\n\n\nRemember how we started a new history at the beginning? If you want to\nsee any of your old histories, click on the history menu button\n\n\nat the top of the histories panel and then select \u201cSaved Histories.\u201d\nThis will give you a list of all the histories you have worked on in\nthis Galaxy server.\n\n\n\n\nGalaxy Workflows\n\n\n\n\nSection 1: Create and run a workflow.\n\n\nThis section will show you two different methods to create a workflow\nand then how to run one.\n\n\nImport the workflow history\n\n\nThis step will show you how to import a history from a remote source\ninto your own workspace. We will be using this history to build a\nworkflow.\n\n\n\n\nFrom the histories menu\n    \n,\n    click \nImport from File\n.\n\n\nEnter\n    \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Workflow-finished-history.tar.gz\n\n    in the URL box.\n\n\nClick \nSubmit\n\n\n\n\nYou might have to wait for a bit, then:\n\n\n\n\nFrom the history menu\n    \n,\n    click \nSaved Histories\n\n\nSelect the history: \nImported: Workflow-finished\n\n\n\n\nWorkflow creation: Method 1\n\n\nWe will create a workflow from an existing history. You can use this\nmethod to make a re-useable analysis from one you\u2019ve already done. i.e.\nYou can perform the analysis once and then create a workflow out of it\nto re-use it on more/new data. We will create a workflow from the\nhistory you imported in step 1 above. The footnote below explains the\nsteps that created this history. These are the steps we will mimic in\nthe workflow.\n\n\nMake sure your current history is the one you imported in Section 2 -\nStep 1 above (\nimported: Workflow_finished.\n) If not, switch to it.\n\n\nNow we will create the workflow.\n\n\n\n\nClick on the histories menu button\n    \n\n    at the top of the history pane.\n\n\nClick \nExtract Workflow\n\n\n\n\nYou will now be shown a page which contains the steps used to create the\nhistory you are extracting from. We use this page to say what to include\nin the workflow. We want everything here so we\u2019ll just accept the\ndefaults and:\n\n\n\n\nChange the Workflow name to something sensible like \u201cBasic Variant\n    Calling Workflow\u201d\n\n\nClick \nCreate Workflow\n\n\n\n\nThe workflow is now accessible via the bottom of the tool pane by\nclicking on All Workflows.\n\n\nSome discussion:\n\n\nHave a look at your workflow. Click on its button in the workflow list.\nIt\u2019s tool interface will appear. You can now run this workflow any time\nyou like with different input datasets. NOTE: The input data sets must\nbe of the same types as the original ones. i.e. Two fastq reads files\nand one fasta reference sequence.\n\n\nMore interesting though is to:\n\n\n\n\nClick on the \nWorkflows\n link in the top menu\n\n\nClick on the down arrow on your workflow\u2019s button.\n\n\nClick \nEdit\n\n\n\n\nA visualisation of your workflow will appear. Note the connections and\nthe steps.\n\n\nNext we\u2019ll go through how to create this workflow using the editor..\n\n\nWorkflow Creation: Method 2\n\n\nWe will now create the same read mapping/variant calling workflow using\nthe editor directly. We need to get some reads and a reference, map the\nreads to the reference using BWA, run Freebayes on the BAM to call the\nvariants, and finally filter the resulting vcf file.\n\n\nStep 1: Create a workflow name and edit space. {#step-1-create-a-workflow-name-and-edit-space}\n\n\n\n\nClick on \nWorkflow\n in Galaxy\u2019s menu.\n\n\nClick on the \nCreate New Workflow\n button.\n\n\nIn the \"Workflow Name\" text box type: \nVariants from scratch\n\n\nClick the \nCreate\n button.\n\n\n\n\nYou\u2019ll now see a list of workflows. Your\u2019s will be in that list.\n\n\nStep 2: Open the editor and place component tools\n\n\n\n\nClick on the down arrow on your workflow\u2019s button.\n\n\nSelect \nEdit\n\n\n\n\nYou\u2019ll be presented with a blank workflow grid.\n\n\nAdd three input datafiles.\n\n\n\n\nIn the Workflow control section of the tool pane, click on \nInputs\n    -> Input dataset\n three times.\n\n\nSpread them out towards the left hand side of the workflow grid by\n    clicking and dragging them around.\n\n\nFor each one, change their name.\n\n\nClick on each input box in turn\n\n\nIn the right hand pane (where the history usually is), change\n    the name to:\n\n\nReference data\n\n\nReads 1\n\n\nReads 2 - respectively.\n\n\n\n\n\n\n\n\n\n\n\n\nAdd in the BWA mapping step.\n\n\n\n\nClick on \nNGS: Mapping -> Map with BWA\n in the tool pane. BWA\n    will be added to the workflow grid.\n\n\nIn the right hand pane (where the history usually is), change the\n    following parameters.\n\n\nChange \u201cWill you select a reference genome from your history or\n    use a built-in index?:\u201d to \nUse a genome from history.\n\n\nNote that the BWA box on the grid changes to match\n    these settings.\n\n\n\n\n\n\n\n\nConnect the tools together.\n\n\n\n\nClick and drag on the output of one of the input dataset tools to\n    each of the input spots in the BWA tool. (Make connections.)\n\n\n\"Reference data\" output to reference to \"Use the following\n    dataset as the reference sequence\"\n\n\n\"Reads 1\" output to \"Select first set of reads\"\n\n\n\"Reads 2\" output to \"Select second set of reads\"\n\n\n\n\n\n\n\n\nAdd in the Freebayes (Variant Calling step.)\n\n\n\n\nClick on \nNGS: Variant Calling -> Freebayes\n\n\nIn the right hand pane, change the following:\n\n\n\"Choose the source for the reference list:\" to \nHistory\n\n\nConnect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.\n\n\nConnect the \"Reference data\" output to \"Freebayes\u2019\" \nUse the\n    following dataset as the reference sequence\n input.\n\n\n\n\n\n\n\n\nIf you\u2019re keen - \nNote: this is optional.\n Also change the following\nparameters the right hand pane for freebayes to make it a bit more\nsensible for variant calling in bacterial genomes.\n\n\n\n\n\"Choose parameter selection level\": \nComplete list of all options\n\n\n\"Set population model\": \nYes\n\n\n\"Set ploidy for the analysis\": \n1\n\n\n\"Set input filters\": \nYes\n\n\n\"Exclude alignments from analysis if they have a mapping quality\n    less than\": \n20\n\n\n\"Exclude alleles from analysis if their supporting base quality is\n    less than\": \n20\n\n\n\"Require at least this fraction of observations \u2026 to evaluate the\n    position\": \n0.9\n\n\n\"Require at least this count of observations .. to evaluate the\n    position\": \n10\n\n\n\"Set population and mappability priors\": \nYes\n\n\n\"Disable incorporation of prior expectations about observations\":\n    \nYes\n\n\n\n\nAdd in the Filter step.\n\n\n\n\nClick on \nFilter and Sort - > Filter\n\n\nConnect \"Freebayes\u2019\" output_vcf to the \"filter\" input.\n\n\nIn the right hand pane, change the following:\n\n\n\"With the following condition\": \nc6 > 500\n\n\n\"Number of header lines to skip\": \n56\n\n\n\n\n\n\n\n\nPhew! We\u2019re nearly done! The only thing left is to select which workflow\noutputs we want to keep in our history. Next to each output for every\ntool is a star. Clicking on the stars will select those files as\nworkflow outputs, everything else will be hidden in the history. In this\ncase we only really want the BAM file and the final variants (vcf file.)\nTherefore:\n\n\nSelect workflow outputs.\n\n\n\n\nClick on the star next to \"Map with BWA\u2019s\" bam file output.\n\n\nClick on the star next to \"Filter\u2019s\" output vcf.\n\n\n\n\nStep 3: Save it!\n\n\nClick on the\n\n\nat the top of the workflow grid and select \nSave\n.\n\n\nCongratulations. You\u2019ve just created a Galaxy workflow.\n\n\nNow to run it!\n\n\nRunning the workflow\n\n\nWe will now make a new history called \"Test\" and run the workflow on\nit\u2019s data.\n\n\nCreate the new history\n\n\n\n\nFrom the Histories Menu, select \nCopy Datasets\n\n\nSelect the 2 x fastq files and the Ecoli .fna file.\n\n\nUnder destination history, enter \nTest\n into \"New History Named:\"\n\n\nClick \nCopy History Items\n button\n\n\nClick on the link to the new history in the green bar at the top of\n    the screen\n\n\n\n\nRun the workflow\n\n\n\n\nOn the tools pane, click \nAll Workflows\n\n\nSelect the \nVariants from scratch\n workflow (or whatever you\n    called it.)\n\n\nGive it the correct files.\n\n\nEcoli ... .fna\n for Reference data\n\n\nbacterial_std_err_1.fastq\n for Reads 1\n\n\nbacterial_std_err_2.fastq\n for Reads 2\n\n\nClick \nRun Workflow\n\n\n\n\nYour workflow will now run. It will send the right files to the right\ntools at the right time to the cluster (compute engine on your machine)\nand wait for them to finish. Watch as they turn yellow then green in\nturn.\n\n\nWhat now?\n\n\nWhere to start? There\u2019s so much you can do with Workflows. You can even\nrun them on multiple file sets from the one setup.\n\n\nThat's it. You now know a bit about the Galaxy interface and how to load\ndata, run tools and view their outputs. For more tutorials, see\n\nhttp://genome.edu.au/learn\n\n\n\n\nDocumentation built with \nMkDocs\n.",
            "title": "Home"
        },
        {
            "location": "/#introduction-to-galaxy",
            "text": "The tutorial is the modified version which was written by  Simon\nGladman  - VLSCI",
            "title": "Introduction to Galaxy"
        },
        {
            "location": "/#background",
            "text": "Galaxy is a web based analysis and workflow platform designed for\nbiologists to analyse their own data. It comes with most of the popular\nbioinformatics tools already installed and ready for use. There are many\nGalaxy servers around the world and some are tailored with specific\ntoolsets and reference data for analysis of human genomics, microbial\ngenomics, proteomics etc.  There are some introductory slides available here .  Basically, the Galaxy interface is separated into 3 parts. The tool list\non the left, the viewing pane in the middle and the analysis and data\nhistory on the right. We will be looking at all 3 parts in this\ntutorial.   This workshop/tutorial will familiarize you with the Galaxy interface.\nIt will cover the following topics:   Logging in to the server  Getting data into galaxy  How to access the tools  Using to use some common tools",
            "title": "Background"
        },
        {
            "location": "/#learning-objectives",
            "text": "At the end of this tutorial you should:  Be able to upload data to a Galaxy server from:\n    -   A file on your local computer\n    -   A file on a remote datastore with an accessible URL.\n    -   A file UCSC genome browser  missing  Be able use tools in Galaxy by:\n    -   Accessing the tool via the tool menu\n    -   Using the tool interface to run the particular tool\n    -   Viewing/accessing the tool output.",
            "title": "Learning Objectives"
        },
        {
            "location": "/#section-1-preparation",
            "text": "The purpose of this section is to get you to log in to the server..    Go to  MDC galaxy production sever    Log in:   Enter your MDC user name  Enter your password  Click  OK",
            "title": "Section 1: Preparation."
        },
        {
            "location": "/#section-2-getting-data-into-galaxy",
            "text": "There are 2 main ways to get your data into Galaxy. We will use each of\nthese methods for 3 files and then work use those 3 files for the rest\nof the workshop.    Start a new history for this workshop. To do this:   Click on the history menu button (the\n      icon)\n    at the top of the Histories panel.  Select  Create New     It is important to note that Galaxy has the concept of \"File Type\" built\nin. This means that each file stored needs to have its type described to\nGalaxy as it is being made available. Examples of file types are: text,\nfasta, fastq, vcf, GFF, Genbank, tabular etc.  We will tell Galaxy what type of file each one is as we upload it.",
            "title": "Section 2: Getting data into Galaxy"
        },
        {
            "location": "/#method-1-upload-a-file-from-your-own-computer",
            "text": "With this method you can get most of the files on your own computer into\nGalaxy. (there is a size limit)    Download the following file to your computer:  to change \n     https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz   From the Galaxy tool panel, click on  Get Data -> Upload\n    File  Click the  Choose File  button  Find and select the  Contig_stats.txt.gz  file you downloaded\n    and click  Open  Set the \"file format\" to  tabular  Click the  Start  button  Once the progress bar reaches 100%, click the  Close  button     The file will now upload to your current history.",
            "title": "Method 1: Upload a file from your own computer"
        },
        {
            "location": "/#method-2-upload-a-file-from-a-url",
            "text": "If a file exists on a web resource somewhere and you know its URL\n(Unique resource location - a web address) you can directly load it into\nGalaxy.    From the tool panel, click on  Get Data -> Upload File   Click on the  Paste/Fetch Data  button  Copy and paste the following web address into the URL/Text box:  to change \n     https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz  Set the file format to  fastqsanger  (not fastqcsanger)  Click  Start  Once the progress bar has reached 100%, click  Close     Note that Galaxy is smart enough to recognize that this is a compressed\nfile and so it will uncompress it as it loads it.",
            "title": "Method 2: Upload a file from a URL"
        },
        {
            "location": "/#method-2-again-get-data-from-a-public-url",
            "text": "Now we are going to upload another file from the remote data source.  Repeat the above for: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna  Note that this file is a  fasta  file and not a  fastqsanger  file.  The DNA sequence of  Staphlococcus aureus MRSA252  will be loaded into\nyour history as a fasta file.",
            "title": "Method 2 (again): Get data from a public URL"
        },
        {
            "location": "/#the-data",
            "text": "Though we aren't going to focus on the contents of these files and what\nthey mean from a bioinformatics standpoint, here is a brief description\nof each one.    Contigs_stats.txt   this file contains a table of summary data from a de novo genome\n    assembly (the process of attempting to recover the full genome\n    of an organism from the short read sequences produced by most\n    DNA sequencing machines. )  The columns contain a lot of information but the ones we will be\n    using indicate the amount of data (or coverage) that went into\n    making up each piece of the final assembly.        bacterial_std_err_1.fastq.gz   This file contains sequence reads as they would come off an\n    Illunina sequencing machine. They are in\n     fastq  format.        MRSA0252.fna   This file contains the genome sequence of  Staphylococcus aureus\n    MRSA252 . It is in\n     fasta  format.",
            "title": "The data"
        },
        {
            "location": "/#section-3-play-with-the-tools",
            "text": "The purpose of this section is to get you used to using the available\ntools in Galaxy and point out some of the more basic manipulation tools.  Firstly however, you\u2019ll notice that two of the files have very long and\nconfusing names. So we might want to change them. To do this we need to\n\u201cedit\u201d the file. So:   Click on the\n     \n    icon (edit) next to the file in the history called:\n     https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq  In the \"Name\" text box, give it a new name. Call it:  Typical Fastq\n    File  Click the  Save  button.   Repeat the process for the MRSA252 fasta file. Rename it to MRSA252.fna  Now that\u2019s better. There was a lot of other functionality hidden behind\nthat edit\n( )\nicon. You can change a file\u2019s data type, convert its format and many\nother things. Feel free to play around with them.  Ok, back to the tools..",
            "title": "Section 3: Play with the tools"
        },
        {
            "location": "/#example-1-histogram-and-summary-statistics",
            "text": "The first thing we are going to do is produce a histogram of contig read\ncoverage depths and calculate the summary statistics from the\nContig_stats.txt file. To do this we need to cut out a couple of\ncolumns, remove a line and then produce a histogram. This will introduce\nsome of the text manipulation tools.  Click on the \nicon of the  Contig_stats.txt  file to have a look at it. Note that\nthere are 18 columns in this file. We want column 1 and column 6. To do\nthis:  1. Cut out column 1 and column 6.   From the tool panel, click on  Text Manipulation -> Cut  and\n    set the following:  Set \"Cut Columns\" to:  c1,c6  \"Delimited by\":  Tab  \"Cut from\":  Contig_stats.txt  Click  Execute   Examine the new file by clicking on it\u2019s \nicon. We now have 2 columns instead of the 18 in the original file.  2. Remove the Header lines of the new file.   From the tool panel, click on  Text Manipulation -> Remove\n    beginning  and set the following:  \"Remove First\":  1  \"from\":  Cut on data1  click  Execute   Note the the new file is the same as the previous one without the header\nline.  3. Make a histogram.   From the tool panel, click on  Graph/Display Data -> Histogram \n    and set the following:  \"Dataset\":  Remove beginning on Data X  \"Numerical column for X axis\":  c2  \"Number of breaks\":  25  \"Plot title\":  Histogram of Contig Coverage  \"Label for X axis\":  Coverage depth  Click  Execute   Click on the \nicon of the histogram to have a look at it. Note there are a few peaks..\nMaybe these correspond to single, double and triple copy number of these\ncontigs.  4. Calculate summary statistics for contig coverage depth.   From the tool panel, click on  Statistics -> Summary\n    Statisitics  and set the following:  \"Summary statistics on\":  Remove beginning on Data X  \"Column or expression\":  c2  Click  Execute   You\u2019ll note that the summary statistics tool failed and is red in the\nhistory. There was an error! If you click on the filename, and then the\nbug \nsymbol, it will tell you what went wrong. (There is a missing python\nlibrary.) At this point, you would normally contact your Galaxy server\nadministrator.",
            "title": "Example 1: Histogram and summary statistics"
        },
        {
            "location": "/#example-2-convert-fastq-to-fasta",
            "text": "This shows how to convert a fastq file to a fasta file. The tool creates\na new file with the converted data.  Converter tool   From the tool panel, click on  Convert Formats -> FASTQ to\n    FASTA  and set the following:  \"FASTQ file to convert\":  Typical Fastq File  Click  Execute   This will have created a new Fasta file called FASTQ to FASTA on data 2.",
            "title": "Example 2: Convert Fastq to Fasta"
        },
        {
            "location": "/#example-3-find-ribosomal-rna-features-in-a-dna-sequence",
            "text": "This example shows how to use a tool called \u201cbarrnap\u201d to search for\nrRNAs in a DNA sequence.  1. Find all of the ribosomal RNA's in a sequence   From the tool panel, click on  Annotation -> barrnap  and set\n    the following:  \"Fasta file\": MRSA252.fna  Click  Execute   A new file called  barrnap on data 3  will be produced. It is a gff3\nfile. (This stands for genome feature format - version 3. It is a file\nformat for describing features contained by a DNA sequence.) Change it\u2019s\nname to something more appropriate (click on the \nicon.) There is also a STDERR output file from this tool - just ignore\nthis one.  Now lets say you only want the lines of the file for the 23S rRNA\nannotations. We can do this using a Filter tool.  2. Filter the annotations to get the 23S RNAs   From the tool panel, click on  Filter and Sort -> Select  and\n    set the following:  \"Select lines from\": (whatever you called the barrnap gff3 output)  \"the pattern\":  23S  (this will look for all the lines in the file\n    that contain \u201c23S\u201d)  Click  Execute   Now you have a gff3 file with just the 23S annotations!",
            "title": "Example 3: Find Ribosomal RNA Features in a DNA Sequence"
        },
        {
            "location": "/#what-now",
            "text": "Remember how we started a new history at the beginning? If you want to\nsee any of your old histories, click on the history menu button \nat the top of the histories panel and then select \u201cSaved Histories.\u201d\nThis will give you a list of all the histories you have worked on in\nthis Galaxy server.",
            "title": "What now?"
        },
        {
            "location": "/#galaxy-workflows",
            "text": "",
            "title": "Galaxy Workflows"
        },
        {
            "location": "/#section-1-create-and-run-a-workflow",
            "text": "This section will show you two different methods to create a workflow\nand then how to run one.",
            "title": "Section 1: Create and run a workflow."
        },
        {
            "location": "/#import-the-workflow-history",
            "text": "This step will show you how to import a history from a remote source\ninto your own workspace. We will be using this history to build a\nworkflow.   From the histories menu\n     ,\n    click  Import from File .  Enter\n     https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Workflow-finished-history.tar.gz \n    in the URL box.  Click  Submit   You might have to wait for a bit, then:   From the history menu\n     ,\n    click  Saved Histories  Select the history:  Imported: Workflow-finished",
            "title": "Import the workflow history"
        },
        {
            "location": "/#workflow-creation-method-1",
            "text": "We will create a workflow from an existing history. You can use this\nmethod to make a re-useable analysis from one you\u2019ve already done. i.e.\nYou can perform the analysis once and then create a workflow out of it\nto re-use it on more/new data. We will create a workflow from the\nhistory you imported in step 1 above. The footnote below explains the\nsteps that created this history. These are the steps we will mimic in\nthe workflow.  Make sure your current history is the one you imported in Section 2 -\nStep 1 above ( imported: Workflow_finished. ) If not, switch to it.  Now we will create the workflow.   Click on the histories menu button\n     \n    at the top of the history pane.  Click  Extract Workflow   You will now be shown a page which contains the steps used to create the\nhistory you are extracting from. We use this page to say what to include\nin the workflow. We want everything here so we\u2019ll just accept the\ndefaults and:   Change the Workflow name to something sensible like \u201cBasic Variant\n    Calling Workflow\u201d  Click  Create Workflow   The workflow is now accessible via the bottom of the tool pane by\nclicking on All Workflows.",
            "title": "Workflow creation: Method 1"
        },
        {
            "location": "/#some-discussion",
            "text": "Have a look at your workflow. Click on its button in the workflow list.\nIt\u2019s tool interface will appear. You can now run this workflow any time\nyou like with different input datasets. NOTE: The input data sets must\nbe of the same types as the original ones. i.e. Two fastq reads files\nand one fasta reference sequence.  More interesting though is to:   Click on the  Workflows  link in the top menu  Click on the down arrow on your workflow\u2019s button.  Click  Edit   A visualisation of your workflow will appear. Note the connections and\nthe steps.  Next we\u2019ll go through how to create this workflow using the editor..",
            "title": "Some discussion:"
        },
        {
            "location": "/#workflow-creation-method-2",
            "text": "We will now create the same read mapping/variant calling workflow using\nthe editor directly. We need to get some reads and a reference, map the\nreads to the reference using BWA, run Freebayes on the BAM to call the\nvariants, and finally filter the resulting vcf file.",
            "title": "Workflow Creation: Method 2"
        },
        {
            "location": "/#step-1-create-a-workflow-name-and-edit-space-step-1-create-a-workflow-name-and-edit-space",
            "text": "Click on  Workflow  in Galaxy\u2019s menu.  Click on the  Create New Workflow  button.  In the \"Workflow Name\" text box type:  Variants from scratch  Click the  Create  button.   You\u2019ll now see a list of workflows. Your\u2019s will be in that list.",
            "title": "Step 1: Create a workflow name and edit space. {#step-1-create-a-workflow-name-and-edit-space}"
        },
        {
            "location": "/#step-2-open-the-editor-and-place-component-tools",
            "text": "Click on the down arrow on your workflow\u2019s button.  Select  Edit   You\u2019ll be presented with a blank workflow grid.  Add three input datafiles.   In the Workflow control section of the tool pane, click on  Inputs\n    -> Input dataset  three times.  Spread them out towards the left hand side of the workflow grid by\n    clicking and dragging them around.  For each one, change their name.  Click on each input box in turn  In the right hand pane (where the history usually is), change\n    the name to:  Reference data  Reads 1  Reads 2 - respectively.       Add in the BWA mapping step.   Click on  NGS: Mapping -> Map with BWA  in the tool pane. BWA\n    will be added to the workflow grid.  In the right hand pane (where the history usually is), change the\n    following parameters.  Change \u201cWill you select a reference genome from your history or\n    use a built-in index?:\u201d to  Use a genome from history.  Note that the BWA box on the grid changes to match\n    these settings.     Connect the tools together.   Click and drag on the output of one of the input dataset tools to\n    each of the input spots in the BWA tool. (Make connections.)  \"Reference data\" output to reference to \"Use the following\n    dataset as the reference sequence\"  \"Reads 1\" output to \"Select first set of reads\"  \"Reads 2\" output to \"Select second set of reads\"     Add in the Freebayes (Variant Calling step.)   Click on  NGS: Variant Calling -> Freebayes  In the right hand pane, change the following:  \"Choose the source for the reference list:\" to  History  Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.  Connect the \"Reference data\" output to \"Freebayes\u2019\"  Use the\n    following dataset as the reference sequence  input.     If you\u2019re keen -  Note: this is optional.  Also change the following\nparameters the right hand pane for freebayes to make it a bit more\nsensible for variant calling in bacterial genomes.   \"Choose parameter selection level\":  Complete list of all options  \"Set population model\":  Yes  \"Set ploidy for the analysis\":  1  \"Set input filters\":  Yes  \"Exclude alignments from analysis if they have a mapping quality\n    less than\":  20  \"Exclude alleles from analysis if their supporting base quality is\n    less than\":  20  \"Require at least this fraction of observations \u2026 to evaluate the\n    position\":  0.9  \"Require at least this count of observations .. to evaluate the\n    position\":  10  \"Set population and mappability priors\":  Yes  \"Disable incorporation of prior expectations about observations\":\n     Yes   Add in the Filter step.   Click on  Filter and Sort - > Filter  Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input.  In the right hand pane, change the following:  \"With the following condition\":  c6 > 500  \"Number of header lines to skip\":  56     Phew! We\u2019re nearly done! The only thing left is to select which workflow\noutputs we want to keep in our history. Next to each output for every\ntool is a star. Clicking on the stars will select those files as\nworkflow outputs, everything else will be hidden in the history. In this\ncase we only really want the BAM file and the final variants (vcf file.)\nTherefore:  Select workflow outputs.   Click on the star next to \"Map with BWA\u2019s\" bam file output.  Click on the star next to \"Filter\u2019s\" output vcf.",
            "title": "Step 2: Open the editor and place component tools"
        },
        {
            "location": "/#step-3-save-it",
            "text": "Click on the \nat the top of the workflow grid and select  Save .  Congratulations. You\u2019ve just created a Galaxy workflow.  Now to run it!",
            "title": "Step 3: Save it!"
        },
        {
            "location": "/#running-the-workflow",
            "text": "We will now make a new history called \"Test\" and run the workflow on\nit\u2019s data.",
            "title": "Running the workflow"
        },
        {
            "location": "/#create-the-new-history",
            "text": "From the Histories Menu, select  Copy Datasets  Select the 2 x fastq files and the Ecoli .fna file.  Under destination history, enter  Test  into \"New History Named:\"  Click  Copy History Items  button  Click on the link to the new history in the green bar at the top of\n    the screen",
            "title": "Create the new history"
        },
        {
            "location": "/#run-the-workflow",
            "text": "On the tools pane, click  All Workflows  Select the  Variants from scratch  workflow (or whatever you\n    called it.)  Give it the correct files.  Ecoli ... .fna  for Reference data  bacterial_std_err_1.fastq  for Reads 1  bacterial_std_err_2.fastq  for Reads 2  Click  Run Workflow   Your workflow will now run. It will send the right files to the right\ntools at the right time to the cluster (compute engine on your machine)\nand wait for them to finish. Watch as they turn yellow then green in\nturn.",
            "title": "Run the workflow"
        },
        {
            "location": "/#what-now_1",
            "text": "Where to start? There\u2019s so much you can do with Workflows. You can even\nrun them on multiple file sets from the one setup.  That's it. You now know a bit about the Galaxy interface and how to load\ndata, run tools and view their outputs. For more tutorials, see http://genome.edu.au/learn   Documentation built with  MkDocs .",
            "title": "What now?"
        }
    ]
}